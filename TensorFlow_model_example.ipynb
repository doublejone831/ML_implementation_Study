{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tensorflow based NeuralNet\n",
    "This python notebook is for studying how to make NeuralNet model using Tensorflow\n",
    "Keras is just used for load the dataset\n",
    "Reference:\n",
    "1. https://www.kaggle.com/code/enriqueabad/using-tensorflow-from-scratch-without-keras/notebook\n",
    "2. https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load MNist Dataset from the keras\n",
    "\"\"\"\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28)\n",
      "[[5]\n",
      " [0]\n",
      " [4]\n",
      " ...\n",
      " [5]\n",
      " [6]\n",
      " [8]]\n",
      "[[7]\n",
      " [2]\n",
      " [1]\n",
      " ...\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Normalize the input to make it inside of range 0 to 1\"\"\"\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(y_train.reshape(-1,1))\n",
    "print(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make label as an one hot vector\n",
    "\"\"\"\n",
    "label_encoder = OneHotEncoder(categories=[np.arange(10)])\n",
    "\"\"\"\n",
    "Need to make 1D array to 2D array So use ndarray.reshape()\n",
    "We use -1 to indicate the length of row or column dynamically\n",
    "Need to use .toarray method to make it as a form of [0,0,0,0,1,0,0,0]\n",
    "\"\"\"\n",
    "y_train = label_encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = label_encoder.fit_transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network without hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make weight tensor\n",
    "We have 784 pixel image so the weight tensor size should be 784 x 10\n",
    "-> 784 pixel and out put is 10\n",
    "tf.Variable => tensor in tensorflow is immutable.\n",
    "So we use variables which is tensor whose value can be changed by running ops on it. \n",
    "\"\"\"\n",
    "# weight_random = tf.Variable(tf.random.uniform([784,10]))\n",
    "weight_random = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make the loss function -> mean squared error loss and cross-entropy loss \n",
    "\"\"\"\n",
    "def loss_MSE(target_y, predicted_y):\n",
    "    return tf.reduce_mean(tf.square(target_y-predicted_y))\n",
    "\n",
    "def loss_CrossEntrophy(target_y, predicted_y):\n",
    "    \"\"\"\n",
    "    This calculate cross entrophy\n",
    "    * operator is element wise multiply -> shape must be same\n",
    "    \"\"\"\n",
    "    return -tf.reduce_sum(tf.reduce_mean(target_y*tf.math.log(predicted_y+ 1e-12),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define Flatten Layer\"\"\"\n",
    "def flatten(t):\n",
    "    \"\"\"\n",
    "    By using np.reshape, reshape tensor to 1D\n",
    "    \"\"\"\n",
    "    return t.reshape(t.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(flatten(x_train).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model\n",
    "Make model using tf.Module makes it easier to build own model\n",
    "tf.Module is Base neural network module class\n",
    "Module  is an named container for tf.Variable s, other tf.Module s and functions which apply to user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(tf.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        Weight starts with zeros \n",
    "        But it is better to start with random\n",
    "        \"\"\"\n",
    "        self.w = tf.Variable(tf.zeros([784,10]))\n",
    "        # self.w = tf.Variable(tf.random.uniform([784,10]))\n",
    "        self.b = tf.Variable(0.0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" \n",
    "        With just call the class, we can predict\n",
    "        y_predicted = Model1()(x_data)\n",
    "        @ operator is matmul\n",
    "        \"\"\"\n",
    "        x = flatten(x)\n",
    "        return tf.nn.softmax((x @ self.w + self.b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,x,y,learning_rate):\n",
    "    \"\"\"\n",
    "    This is training function\n",
    "    For gradient I use GradientTape\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as t:\n",
    "        # make prediction and calculate the loss\n",
    "        current_loss = loss_CrossEntrophy(y,model(x))\n",
    "\n",
    "    #calculate the gradient. here calculate the weight and bias\n",
    "    dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "\n",
    "    #update the wieght and bias. Here I just use gradient descent with learning rate(or step size)\n",
    "    model.w.assign_sub(learning_rate*dw)\n",
    "    model.b.assign_sub(learning_rate*db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 0 = 2.0969409942626953. Test loss = 2.09299898147583\n",
      "Training loss in epoch 1 = 1.92383873462677. Test loss = 1.916506290435791\n",
      "Training loss in epoch 2 = 1.775111436843872. Test loss = 1.764846682548523\n",
      "Training loss in epoch 3 = 1.647637128829956. Test loss = 1.634881615638733\n",
      "Training loss in epoch 4 = 1.5384283065795898. Test loss = 1.5235811471939087\n",
      "Training loss in epoch 5 = 1.444663405418396. Test loss = 1.428073525428772\n",
      "Training loss in epoch 6 = 1.3638346195220947. Test loss = 1.345796823501587\n",
      "Training loss in epoch 7 = 1.2937922477722168. Test loss = 1.2745518684387207\n",
      "Training loss in epoch 8 = 1.232735276222229. Test loss = 1.2124978303909302\n",
      "Training loss in epoch 9 = 1.1791810989379883. Test loss = 1.1581134796142578\n",
      "Training loss in epoch 10 = 1.131913423538208. Test loss = 1.1101539134979248\n",
      "Training loss in epoch 11 = 1.0899404287338257. Test loss = 1.0676029920578003\n",
      "Training loss in epoch 12 = 1.0524502992630005. Test loss = 1.0296298265457153\n",
      "Training loss in epoch 13 = 1.01878023147583. Test loss = 0.9955543279647827\n",
      "Training loss in epoch 14 = 0.9883840680122375. Test loss = 0.9648177623748779\n",
      "Training loss in epoch 15 = 0.9608103036880493. Test loss = 0.936957836151123\n",
      "Training loss in epoch 16 = 0.93568354845047. Test loss = 0.911591112613678\n",
      "Training loss in epoch 17 = 0.9126911163330078. Test loss = 0.8883968591690063\n",
      "Training loss in epoch 18 = 0.8915703296661377. Test loss = 0.8671057224273682\n",
      "Training loss in epoch 19 = 0.8720966577529907. Test loss = 0.847490668296814\n",
      "Training loss in epoch 20 = 0.8540825843811035. Test loss = 0.8293579816818237\n",
      "Training loss in epoch 21 = 0.8373656272888184. Test loss = 0.8125429153442383\n",
      "Training loss in epoch 22 = 0.8218072056770325. Test loss = 0.7969033122062683\n",
      "Training loss in epoch 23 = 0.8072879910469055. Test loss = 0.7823177576065063\n",
      "Training loss in epoch 24 = 0.793703556060791. Test loss = 0.7686794996261597\n",
      "Training loss in epoch 25 = 0.780963659286499. Test loss = 0.7558972239494324\n",
      "Training loss in epoch 26 = 0.768988847732544. Test loss = 0.743889331817627\n",
      "Training loss in epoch 27 = 0.757709264755249. Test loss = 0.732585608959198\n",
      "Training loss in epoch 28 = 0.7470648288726807. Test loss = 0.7219234108924866\n",
      "Training loss in epoch 29 = 0.7370003461837769. Test loss = 0.7118477821350098\n",
      "Training loss in epoch 30 = 0.7274675369262695. Test loss = 0.7023096084594727\n",
      "Training loss in epoch 31 = 0.7184235453605652. Test loss = 0.6932651400566101\n",
      "Training loss in epoch 32 = 0.7098307609558105. Test loss = 0.6846754550933838\n",
      "Training loss in epoch 33 = 0.7016537189483643. Test loss = 0.676505446434021\n",
      "Training loss in epoch 34 = 0.6938616037368774. Test loss = 0.6687238216400146\n",
      "Training loss in epoch 35 = 0.686427116394043. Test loss = 0.6613024473190308\n",
      "Training loss in epoch 36 = 0.6793245077133179. Test loss = 0.6542153358459473\n",
      "Training loss in epoch 37 = 0.6725307106971741. Test loss = 0.6474394798278809\n",
      "Training loss in epoch 38 = 0.6660250425338745. Test loss = 0.640953779220581\n",
      "Training loss in epoch 39 = 0.659788966178894. Test loss = 0.6347390413284302\n",
      "Training loss in epoch 40 = 0.6538044214248657. Test loss = 0.6287778615951538\n",
      "Training loss in epoch 41 = 0.6480563879013062. Test loss = 0.623054027557373\n",
      "Training loss in epoch 42 = 0.6425300240516663. Test loss = 0.6175529956817627\n",
      "Training loss in epoch 43 = 0.6372119188308716. Test loss = 0.6122614145278931\n",
      "Training loss in epoch 44 = 0.6320898532867432. Test loss = 0.6071668267250061\n",
      "Training loss in epoch 45 = 0.6271522045135498. Test loss = 0.6022578477859497\n",
      "Training loss in epoch 46 = 0.622389554977417. Test loss = 0.5975238084793091\n",
      "Training loss in epoch 47 = 0.6177912950515747. Test loss = 0.5929551124572754\n",
      "Training loss in epoch 48 = 0.6133490800857544. Test loss = 0.5885426998138428\n",
      "Training loss in epoch 49 = 0.6090542078018188. Test loss = 0.5842784643173218\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actual Training part\n",
    "\"\"\"\n",
    "for i_epochs in range(50):\n",
    "    #train the model\n",
    "    train(model, x_train, y_train, learning_rate=0.2)\n",
    "\n",
    "    #calculate loss from training\n",
    "    train_loss = loss_CrossEntrophy(y_train, model(x_train))\n",
    "    test_loss = loss_CrossEntrophy(y_test, model(x_test))\n",
    "\n",
    "    #print result of trained model\n",
    "    print(f\"Training loss in epoch {i_epochs} = {train_loss.numpy()}. Test loss = {test_loss.numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I face the challenge that if i do with the full batch, the RAM has been exceeded and system got down.\n",
    "To solve this I have to make the model that has train with mini batch or some other method\n",
    "-> Solved: it was error that my y_train was not in on one hot vector array. It was (idx,category) format.\n",
    "So I changed it with using .toarray and It works well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net with a hidden layer\n",
    "include the hidden layer of 100 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(tf.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #input layer\n",
    "        self.w0 = tf.Variable(tf.zeros([784,100]))\n",
    "        self.b0 = tf.Variable(0.0)\n",
    "\n",
    "        #output of input layer is hidden layer. Here its size is 100\n",
    "\n",
    "        #output layer\n",
    "        self.w1 = tf.Variable(tf.zeros([100,10]))\n",
    "        self.b1 = tf.Variable(0.0)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        x0 = flatten(x0)\n",
    "        x1 = tf.nn.sigmoid(x0 @ self.w0 + self.b0)\n",
    "        return tf.nn.softmax(x1 @ self.w1 + self.b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hidden(model, x, y, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss_CrossEntrophy(y,model(x))\n",
    "    \n",
    "    dw0, db0, dw1, db1 = t.gradient(current_loss, [model.w0, model.b0, model.w1, model.b1])\n",
    "\n",
    "    #input layer\n",
    "    model.w0.assign_sub(learning_rate*dw0)\n",
    "    model.b0.assign_sub(learning_rate*db0)\n",
    "\n",
    "    #output layer\n",
    "    model.w1.assign_sub(learning_rate*dw1)\n",
    "    model.b1.assign_sub(learning_rate*db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 0 = 2.3015265464782715.   Test loss = 2.3014349937438965\n",
      "Training loss in epoch 1 = 2.3012466430664062.   Test loss = 2.301133871078491\n",
      "Training loss in epoch 2 = 2.3011817932128906.   Test loss = 2.301055431365967\n",
      "Training loss in epoch 3 = 2.301170825958252.   Test loss = 2.301028251647949\n",
      "Training loss in epoch 4 = 2.301156997680664.   Test loss = 2.3010177612304688\n",
      "Training loss in epoch 5 = 2.301154136657715.   Test loss = 2.301011562347412\n",
      "Training loss in epoch 6 = 2.3011622428894043.   Test loss = 2.3010072708129883\n",
      "Training loss in epoch 7 = 2.30117130279541.   Test loss = 2.301003932952881\n",
      "Training loss in epoch 8 = 2.301147699356079.   Test loss = 2.30100154876709\n",
      "Training loss in epoch 9 = 2.301157236099243.   Test loss = 2.300999164581299\n",
      "Training loss in epoch 10 = 2.3011622428894043.   Test loss = 2.3009955883026123\n",
      "Training loss in epoch 11 = 2.3011484146118164.   Test loss = 2.300992965698242\n",
      "Training loss in epoch 12 = 2.3011581897735596.   Test loss = 2.300990104675293\n",
      "Training loss in epoch 13 = 2.3011579513549805.   Test loss = 2.3009886741638184\n",
      "Training loss in epoch 14 = 2.3011465072631836.   Test loss = 2.3009862899780273\n",
      "Training loss in epoch 15 = 2.301152229309082.   Test loss = 2.300983428955078\n",
      "Training loss in epoch 16 = 2.3011415004730225.   Test loss = 2.300980567932129\n",
      "Training loss in epoch 17 = 2.301133632659912.   Test loss = 2.3009769916534424\n",
      "Training loss in epoch 18 = 2.301140308380127.   Test loss = 2.300973892211914\n",
      "Training loss in epoch 19 = 2.3011345863342285.   Test loss = 2.3009707927703857\n",
      "Training loss in epoch 20 = 2.301137924194336.   Test loss = 2.3009679317474365\n",
      "Training loss in epoch 21 = 2.301138401031494.   Test loss = 2.3009657859802246\n",
      "Training loss in epoch 22 = 2.3011324405670166.   Test loss = 2.300963878631592\n",
      "Training loss in epoch 23 = 2.301131248474121.   Test loss = 2.3009612560272217\n",
      "Training loss in epoch 24 = 2.3011255264282227.   Test loss = 2.3009586334228516\n",
      "Training loss in epoch 25 = 2.3011200428009033.   Test loss = 2.3009562492370605\n",
      "Training loss in epoch 26 = 2.3011221885681152.   Test loss = 2.3009536266326904\n",
      "Training loss in epoch 27 = 2.3011202812194824.   Test loss = 2.300950527191162\n",
      "Training loss in epoch 28 = 2.3011155128479004.   Test loss = 2.300947666168213\n",
      "Training loss in epoch 29 = 2.301114559173584.   Test loss = 2.3009445667266846\n",
      "Training loss in epoch 30 = 2.301107406616211.   Test loss = 2.3009414672851562\n",
      "Training loss in epoch 31 = 2.3011038303375244.   Test loss = 2.3009378910064697\n",
      "Training loss in epoch 32 = 2.3011045455932617.   Test loss = 2.3009350299835205\n",
      "Training loss in epoch 33 = 2.3011014461517334.   Test loss = 2.300931930541992\n",
      "Training loss in epoch 34 = 2.3011064529418945.   Test loss = 2.3009283542633057\n",
      "Training loss in epoch 35 = 2.301105499267578.   Test loss = 2.3009257316589355\n",
      "Training loss in epoch 36 = 2.3010993003845215.   Test loss = 2.300922393798828\n",
      "Training loss in epoch 37 = 2.3010964393615723.   Test loss = 2.300919532775879\n",
      "Training loss in epoch 38 = 2.301086187362671.   Test loss = 2.3009166717529297\n",
      "Training loss in epoch 39 = 2.3010811805725098.   Test loss = 2.3009133338928223\n",
      "Training loss in epoch 40 = 2.3010778427124023.   Test loss = 2.300910711288452\n",
      "Training loss in epoch 41 = 2.3010716438293457.   Test loss = 2.300907611846924\n",
      "Training loss in epoch 42 = 2.3010733127593994.   Test loss = 2.3009045124053955\n",
      "Training loss in epoch 43 = 2.301070213317871.   Test loss = 2.3009016513824463\n",
      "Training loss in epoch 44 = 2.3010663986206055.   Test loss = 2.300898790359497\n",
      "Training loss in epoch 45 = 2.3010644912719727.   Test loss = 2.300896167755127\n",
      "Training loss in epoch 46 = 2.3010571002960205.   Test loss = 2.3008933067321777\n",
      "Training loss in epoch 47 = 2.3010542392730713.   Test loss = 2.3008904457092285\n",
      "Training loss in epoch 48 = 2.30104923248291.   Test loss = 2.3008878231048584\n",
      "Training loss in epoch 49 = 2.301044464111328.   Test loss = 2.300884962081909\n"
     ]
    }
   ],
   "source": [
    "Model_hidden = Model2()\n",
    "\n",
    "for i_epochs in range(50):\n",
    "    train_hidden(Model_hidden,x_train,y_train,learning_rate=0.2)\n",
    "    train_loss = loss_CrossEntrophy(y_train,Model_hidden(x_train))\n",
    "    test_loss = loss_CrossEntrophy(y_test,Model_hidden(x_test))\n",
    "    print(f\"Training loss in epoch {i_epochs} = {train_loss.numpy()}.   Test loss = {test_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(784, 100) dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checking wieght trained\n",
    "\"\"\"\n",
    "print(Model_hidden.w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(tf.Module):\n",
    "    \"\"\"\n",
    "    Model initialize all the weights with random\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #input layer\n",
    "        self.w0 = tf.Variable(tf.random.uniform([784, 100]))\n",
    "        self.b0 = tf.Variable(0.0)\n",
    "\n",
    "        #output of input layer is hidden layer here its size is 100\n",
    "\n",
    "        #output layer\n",
    "        self.w1 = tf.Variable(tf.random.uniform([100, 10]))\n",
    "        self.b1 = tf.Variable(0.0)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        x0 = flatten(x0)\n",
    "        x1 = tf.nn.sigmoid(x0 @ self.w0 + self.b0)\n",
    "        return tf.nn.softmax(x1 @ self.w1 + self.b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 0 = 3.8706977367401123.   Test loss = 3.846771001815796\n",
      "Training loss in epoch 1 = 3.3753533363342285.   Test loss = 3.3750085830688477\n",
      "Training loss in epoch 2 = 2.820781946182251.   Test loss = 2.8151206970214844\n",
      "Training loss in epoch 3 = 2.581589460372925.   Test loss = 2.5788307189941406\n",
      "Training loss in epoch 4 = 2.402167797088623.   Test loss = 2.4005239009857178\n",
      "Training loss in epoch 5 = 2.3747880458831787.   Test loss = 2.377129554748535\n",
      "Training loss in epoch 6 = 2.3600807189941406.   Test loss = 2.3586246967315674\n",
      "Training loss in epoch 7 = 2.352860927581787.   Test loss = 2.3544089794158936\n",
      "Training loss in epoch 8 = 2.3531219959259033.   Test loss = 2.3520333766937256\n",
      "Training loss in epoch 9 = 2.344426155090332.   Test loss = 2.345597743988037\n",
      "Training loss in epoch 10 = 2.349003791809082.   Test loss = 2.3482115268707275\n",
      "Training loss in epoch 11 = 2.339592695236206.   Test loss = 2.340461015701294\n",
      "Training loss in epoch 12 = 2.346315383911133.   Test loss = 2.3457391262054443\n",
      "Training loss in epoch 13 = 2.336440086364746.   Test loss = 2.337089776992798\n",
      "Training loss in epoch 14 = 2.344520330429077.   Test loss = 2.3440535068511963\n",
      "Training loss in epoch 15 = 2.334239959716797.   Test loss = 2.3347396850585938\n",
      "Training loss in epoch 16 = 2.343200445175171.   Test loss = 2.3428595066070557\n",
      "Training loss in epoch 17 = 2.3326520919799805.   Test loss = 2.3330273628234863\n",
      "Training loss in epoch 18 = 2.3422176837921143.   Test loss = 2.3419880867004395\n",
      "Training loss in epoch 19 = 2.33147931098938.   Test loss = 2.3317341804504395\n",
      "Training loss in epoch 20 = 2.3414955139160156.   Test loss = 2.3413290977478027\n",
      "Training loss in epoch 21 = 2.3305540084838867.   Test loss = 2.3307316303253174\n",
      "Training loss in epoch 22 = 2.340942859649658.   Test loss = 2.340832471847534\n",
      "Training loss in epoch 23 = 2.329869270324707.   Test loss = 2.3299436569213867\n",
      "Training loss in epoch 24 = 2.340484857559204.   Test loss = 2.340449810028076\n",
      "Training loss in epoch 25 = 2.3293309211730957.   Test loss = 2.3293070793151855\n",
      "Training loss in epoch 26 = 2.3401098251342773.   Test loss = 2.3401589393615723\n",
      "Training loss in epoch 27 = 2.3288931846618652.   Test loss = 2.328798294067383\n",
      "Training loss in epoch 28 = 2.339787483215332.   Test loss = 2.339928388595581\n",
      "Training loss in epoch 29 = 2.3285038471221924.   Test loss = 2.3283770084381104\n",
      "Training loss in epoch 30 = 2.339595317840576.   Test loss = 2.3397483825683594\n",
      "Training loss in epoch 31 = 2.3282389640808105.   Test loss = 2.328035354614258\n",
      "Training loss in epoch 32 = 2.3394360542297363.   Test loss = 2.339604377746582\n",
      "Training loss in epoch 33 = 2.3279714584350586.   Test loss = 2.327744245529175\n",
      "Training loss in epoch 34 = 2.339294195175171.   Test loss = 2.3394901752471924\n",
      "Training loss in epoch 35 = 2.3277828693389893.   Test loss = 2.3275065422058105\n",
      "Training loss in epoch 36 = 2.339142084121704.   Test loss = 2.3394055366516113\n",
      "Training loss in epoch 37 = 2.3276238441467285.   Test loss = 2.327299118041992\n",
      "Training loss in epoch 38 = 2.339066743850708.   Test loss = 2.3393404483795166\n",
      "Training loss in epoch 39 = 2.3274903297424316.   Test loss = 2.327134609222412\n",
      "Training loss in epoch 40 = 2.3389759063720703.   Test loss = 2.3392863273620605\n",
      "Training loss in epoch 41 = 2.327360153198242.   Test loss = 2.326991081237793\n",
      "Training loss in epoch 42 = 2.338886260986328.   Test loss = 2.3392486572265625\n",
      "Training loss in epoch 43 = 2.3272907733917236.   Test loss = 2.326869487762451\n",
      "Training loss in epoch 44 = 2.3388569355010986.   Test loss = 2.339212417602539\n",
      "Training loss in epoch 45 = 2.327204704284668.   Test loss = 2.326768636703491\n",
      "Training loss in epoch 46 = 2.338778018951416.   Test loss = 2.339190721511841\n",
      "Training loss in epoch 47 = 2.3271536827087402.   Test loss = 2.3266761302948\n",
      "Training loss in epoch 48 = 2.338740825653076.   Test loss = 2.3391661643981934\n",
      "Training loss in epoch 49 = 2.327089548110962.   Test loss = 2.3266000747680664\n"
     ]
    }
   ],
   "source": [
    "Model_hidden_random = Model3()\n",
    "\n",
    "for i_epochs in range(50):\n",
    "    train_hidden(Model_hidden_random,x_train,y_train,learning_rate=0.2)\n",
    "    train_loss = loss_CrossEntrophy(y_train,Model_hidden_random(x_train))\n",
    "    test_loss = loss_CrossEntrophy(y_test,Model_hidden_random(x_test))\n",
    "    print(f\"Training loss in epoch {i_epochs} = {train_loss.numpy()}.   Test loss = {test_loss.numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss of model with hidden layer is bigger because model is not converged. if train with better algorithm(efficient algorithm than gradient descent) or more epoch, model performance will better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
