{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tensorflow based NeuralNet without any libs\n",
    "This python notebook is for studying how to make NeuralNet model using Tensorflow\n",
    "Keras is just used for load the dataset\n",
    "Reference:\n",
    "1. https://www.kaggle.com/code/enriqueabad/using-tensorflow-from-scratch-without-keras/notebook\n",
    "2. https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load MNist Dataset from the keras\n",
    "\"\"\"\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28)\n",
      "(60000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Normalize the input to make it inside of range 0 to 1\"\"\"\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(y_train.reshape(-1,1).shape)\n",
    "print(y_test.reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make label as an one hot vector\n",
    "\"\"\"\n",
    "label_encoder = OneHotEncoder(categories=[np.arange(10)])\n",
    "\"\"\"\n",
    "Need to make 1D array to 2D array So use ndarray.reshape()\n",
    "We use -1 to indicate the length of row or column dynamically\n",
    "\"\"\"\n",
    "y_train = label_encoder.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = label_encoder.fit_transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network without hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make weight tensor\n",
    "We have 784 pixel image so the weight tensor size should be 784 x 10\n",
    "-> 784 pixel and out put is 10\n",
    "tf.Variable => tensor in tensorflow is immutable.\n",
    "So we use variables which is tensor whose value can be changed by running ops on it. \n",
    "\"\"\"\n",
    "# weight_random = tf.Variable(tf.random.uniform([784,10]))\n",
    "weight_random = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make the loss function -> mean squared error loss and cross-entropy loss \n",
    "\"\"\"\n",
    "def loss_MSE(target_y, predicted_y):\n",
    "    return tf.reduce_mean(tf.square(target_y-predicted_y))\n",
    "\n",
    "def loss_CrossEntrophy(target_y, predicted_y):\n",
    "    p = tf.reduce_mean(np.multiply(target_y,tf.math.log(predicted_y+1e-12)))\n",
    "    return -tf.reduce_sum(p,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define Flatten Layer\"\"\"\n",
    "def flatten(t):\n",
    "    \"\"\"\n",
    "    By using np.reshape, reshape tensor to 1D\n",
    "    \"\"\"\n",
    "    return t.reshape(t.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(flatten(x_train).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model\n",
    "Make model using tf.Module makes it easier to build own model\n",
    "tf.Module is Base neural network module class\n",
    "Module  is an named container for tf.Variable s, other tf.Module s and functions which apply to user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(tf.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        Weight starts with zeros \n",
    "        But it is better to start with random\n",
    "\n",
    "        \"\"\"\n",
    "        self.w = tf.Variable(tf.zeros([784,10]))\n",
    "        #self.w = tf.Variable(tf.random.uniform[784,10])\n",
    "        self.b = tf.Variable(0.0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" \n",
    "        With just call the class, we can predict\n",
    "        y_predicted = Model1()(x_data)\n",
    "        \"\"\"\n",
    "        x = flatten(x)\n",
    "        return tf.nn.softmax((x @ self.w + self.b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,x,y,learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss_CrossEntrophy(y,model(x))\n",
    "    \n",
    "    dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "\n",
    "    model.w.assign_sub(learning_rate*dw)\n",
    "    model.b.assign_sub(learning_rate*db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse matrix is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i_epochs \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m     train(model, x_train, y_train, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m     train_loss \u001b[39m=\u001b[39m loss_CrossEntrophy(y_train, model(x_train))\n\u001b[0;32m      4\u001b[0m     test_loss \u001b[39m=\u001b[39m loss_CrossEntrophy(y_test, model(x_test))\n",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, x, y, learning_rate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model,x,y,learning_rate):\n\u001b[0;32m      2\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m----> 3\u001b[0m         current_loss \u001b[39m=\u001b[39m loss_CrossEntrophy(y,model(x))\n\u001b[0;32m      5\u001b[0m     dw, db \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mgradient(current_loss, [model\u001b[39m.\u001b[39mw, model\u001b[39m.\u001b[39mb])\n\u001b[0;32m      7\u001b[0m     model\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39massign_sub(learning_rate\u001b[39m*\u001b[39mdw)\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mloss_CrossEntrophy\u001b[1;34m(target_y, predicted_y)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_CrossEntrophy\u001b[39m(target_y, predicted_y):\n\u001b[1;32m----> 8\u001b[0m     p \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(np\u001b[39m.\u001b[39mmultiply(target_y\u001b[39m+\u001b[39;49m\u001b[39m1e-12\u001b[39;49m,tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(predicted_y)))\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(p,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\scipy\\sparse\\_base.py:467\u001b[0m, in \u001b[0;36mspmatrix.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Now we would add this scalar to every element.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39madding a nonzero scalar to a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    468\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39msparse matrix is not supported\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    469\u001b[0m \u001b[39melif\u001b[39;00m isspmatrix(other):\n\u001b[0;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse matrix is not supported"
     ]
    }
   ],
   "source": [
    "for i_epochs in tqdm(range(50)):\n",
    "    train(model, x_train, y_train, learning_rate=0.2)\n",
    "    train_loss = loss_CrossEntrophy(y_train, model(x_train))\n",
    "    test_loss = loss_CrossEntrophy(y_test, model(x_test))\n",
    "    print(f\"Training loss in epoch {i_epochs} = {train_loss.numpy()}. Test loss = {test_loss.numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I face the challenge that if i do with the full batch, the RAM has been exceeded and system got down.\n",
    "To solve this I have to make the model that has train with mini batch or some other method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
